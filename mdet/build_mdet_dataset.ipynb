{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import galsim\n",
    "from galcheat.utilities import mag2counts, mean_sky_level\n",
    "import btk\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "from mdet.make_mdet_seg import dcut_reformat, create_metadata\n",
    "from mdet.cat_build_tool import convert_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e1e2_to_ephi(e1,e2):\n",
    "    \n",
    "    pa = np.arctan(e2/e1)\n",
    "    \n",
    "    return pa\n",
    "\n",
    "L0 = 3.0128e28\n",
    "\n",
    "def get_bbox(mask):\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    return rmin-4, rmax+4, cmin-4, cmax+4\n",
    "\n",
    "def dcut_reformat(cat, only_bttri=True):\n",
    "    \n",
    "    for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "        cat[f'{band}_ab'] = cat[f'{band}_ab']\n",
    "    \n",
    "    # only bulge to total ratio i-band from the cosmoDC2 GCR Catalog\n",
    "    if only_bttri:\n",
    "        total_flux = L0 * 10**(-0.4*cat[f'i_ab'])\n",
    "        bulge_to_total_ratio = cat[f'fluxnorm_bulge']\n",
    "\n",
    "        cat[f'fluxnorm_bulge_i'] = total_flux * bulge_to_total_ratio\n",
    "        cat[f'fluxnorm_disk_i'] = total_flux * (1-bulge_to_total_ratio)\n",
    "        cat[f'fluxnorm_agn_i'] = np.zeros(total_flux.shape)\n",
    "    else:\n",
    "        for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "            total_flux = L0 * 10**(-0.4*cat[f'{band}_ab'])\n",
    "            bulge_to_total_ratio = cat['fluxnorm_bulge']\n",
    "\n",
    "            cat[f'fluxnorm_bulge_{band}'] = total_flux * bulge_to_total_ratio\n",
    "            cat[f'fluxnorm_disk_{band}'] = total_flux * (1-bulge_to_total_ratio)\n",
    "            cat[f'fluxnorm_agn_{band}'] = np.zeros(total_flux.shape)\n",
    "\n",
    "    #cat['a_b'] = cat['size_bulge_true']\n",
    "    #cat['b_b'] = cat['size_minor_bulge_true']\n",
    "\n",
    "    #cat['a_d'] = cat['size_disk_true']\n",
    "    #cat['b_d'] = cat['size_minor_disk_true']\n",
    "\n",
    "    cat['pa_bulge'] = e1e2_to_ephi(cat['ellipticity_1_bulge_true'],cat['ellipticity_2_bulge_true']) * 180.0/np.pi\n",
    "\n",
    "    cat['pa_disk'] = e1e2_to_ephi(cat['ellipticity_1_disk_true'],cat['ellipticity_2_disk_true']) * 180.0/np.pi\n",
    "    \n",
    "    cat['pa_tot'] = e1e2_to_ephi(cat['ellipticity_1_true'],cat['ellipticity_2_true']) * 180.0/np.pi\n",
    "\n",
    "    cat['g1'] = cat['shear_1']\n",
    "    cat['g2'] = cat['shear_2']\n",
    "    \n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_galaxy(entry, survey, filt, no_disk= False, no_bulge = False, no_agn = True):\n",
    "    components = []\n",
    "    total_flux = mag2counts(entry[filt + \"_ab\"], survey.name, filt).to_value(\"electron\")\n",
    "    # Calculate the flux of each component in detected electrons.\n",
    "    total_fluxnorm = entry[\"fluxnorm_disk_\"+filt] + entry[\"fluxnorm_bulge_\"+filt] + entry[\"fluxnorm_agn_\"+filt]\n",
    "    disk_flux = 0.0 if no_disk else entry[\"fluxnorm_disk_\"+filt] / total_fluxnorm * total_flux\n",
    "    bulge_flux = 0.0 if no_bulge else entry[\"fluxnorm_bulge_\"+filt] / total_fluxnorm * total_flux\n",
    "    agn_flux = 0.0 if no_agn else entry[\"fluxnorm_agn_\"+filt] / total_fluxnorm * total_flux\n",
    "\n",
    "    if disk_flux + bulge_flux + agn_flux == 0:\n",
    "        raise SourceNotVisible\n",
    "\n",
    "    if disk_flux > 0:\n",
    "        a_d, b_d = entry[\"a_d\"], entry[\"b_d\"]\n",
    "        disk_hlr_arcsecs=a_d\n",
    "        \n",
    "        \n",
    "        disk_q = b_d/a_d\n",
    "        pa = np.pi*(entry['pa_disk']+entry['sim_angle'])/180\n",
    "        \n",
    "        epsilon_disk = (1 - disk_q) / (1 + disk_q)\n",
    "        \n",
    "        e1_disk = epsilon_disk * np.cos(2 * pa)\n",
    "        e2_disk = epsilon_disk * np.sin(2 * pa)\n",
    "\n",
    "        disk = galsim.Exponential(flux=disk_flux, half_light_radius=disk_hlr_arcsecs).shear(\n",
    "            e1=-e1_disk, e2=e2_disk\n",
    "        )\n",
    "        \n",
    "        components.append(disk)\n",
    "        \n",
    "        \n",
    "    if bulge_flux > 0:\n",
    "        a_b, b_b = entry[\"a_b\"], entry[\"b_b\"]\n",
    "        bulge_hlr_arcsecs = np.sqrt(a_b * b_b)\n",
    "\n",
    "        bulge_q = b_b/a_b\n",
    "\n",
    "        pa = np.pi*(entry['pa_bulge']+entry['sim_angle'])/180\n",
    "\n",
    "        \n",
    "        epsilon_bulge = (1 - bulge_q) / (1 + bulge_q)\n",
    "        \n",
    "        e1_bulge = epsilon_bulge * np.cos(2 * pa)\n",
    "        e2_bulge = epsilon_bulge * np.sin(2 * pa)\n",
    "        \n",
    "        bulge = galsim.DeVaucouleurs(flux=bulge_flux, half_light_radius=bulge_hlr_arcsecs).shear(\n",
    "           e1=-e1_bulge, e2=e2_bulge\n",
    "        )\n",
    "        components.append(bulge)\n",
    "\n",
    "    if agn_flux > 0:\n",
    "        agn = galsim.Gaussian(flux=agn_flux, sigma=1e-8)\n",
    "        components.append(agn)\n",
    "\n",
    "    profile = galsim.Add(components)\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seg(entry, survey, filt, lvl,nx=128,ny=128):\n",
    "    psf = survey.get_filter(filt).psf\n",
    "    sky_level = mean_sky_level(survey, filt).to_value('electron') # gain = 1\n",
    "    obj_type = entry['truth_type'] # 1 for galaxies, 2 for stars\n",
    "    im = None\n",
    "    if obj_type == 1:\n",
    "        gal = make_galaxy(entry, survey, survey.get_filter(filt))\n",
    "        #gal = gal.shear(g1=entry[\"g1\"], g2=entry[\"g2\"])\n",
    "        conv_gal = galsim.Convolve(gal, psf)\n",
    "        im = conv_gal.drawImage(\n",
    "            nx=nx,\n",
    "            ny=nx,\n",
    "            scale=survey.pixel_scale.to_value(\"arcsec\")\n",
    "        )\n",
    "    '''else:\n",
    "        star, gsparams, isbright = make_star(entry, survey, survey.get_filter(filt))\n",
    "        max_n_photons = 10_000_000\n",
    "        # 0 means use the flux for n_photons \n",
    "        mag =entry['mag_'+filt]\n",
    "        flux = mag2counts(mag,survey,filt).to_value(\"electron\")\n",
    "        n_photons = 0 if flux < max_n_photons else max_n_photons\n",
    "        #n_photons = 0 if entry[f'flux_{filt}'] < max_n_photons else max_n_photons\n",
    "        conv_star = galsim.Convolve(star, psf)\n",
    "        im = conv_star.drawImage(\n",
    "            nx=nx,\n",
    "            ny=nx,\n",
    "            scale=survey.pixel_scale.to_value(\"arcsec\"),\n",
    "            method=\"phot\",\n",
    "            n_photons=n_photons,\n",
    "            poisson_flux=True,\n",
    "            maxN=1_000_000,  # shoot in batches this size\n",
    "            rng=grng\n",
    "        )'''\n",
    "        \n",
    "    imd = np.expand_dims(np.expand_dims(im.array,0),0)\n",
    "    # thresh for mask set relative to the bg noise level which is what sigma_noise is\n",
    "    # so lower the thresh for the star to include more of its light\n",
    "    # so lower sigma_noise, bigger masks and higher lvl, smaller masks bc it'll only capture very brightest central part of star\n",
    "    if obj_type == 2: # if star, \n",
    "        segs = btk.metrics.utils.get_segmentation(imd, sky_level, sigma_noise=lvl)\n",
    "    else:\n",
    "        segs = btk.metrics.utils.get_segmentation(imd, sky_level, sigma_noise=lvl)        \n",
    "    return segs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_im(entry, survey, filt, lvl,nx=128,ny=128):\n",
    "    psf = survey.get_filter(filt).psf\n",
    "    sky_level = mean_sky_level(survey.name, filt).to_value('electron') # gain = 1\n",
    "    obj_type = entry['truth_type'] # 1 for galaxies, 2 for stars\n",
    "    im = None\n",
    "    if obj_type == 1:\n",
    "        gal = make_galaxy(entry, survey, filt)\n",
    "        #gal = gal.shear(g1=entry[\"g1\"], g2=entry[\"g2\"])\n",
    "        conv_gal = galsim.Convolve(gal, psf)\n",
    "        im = conv_gal.drawImage(\n",
    "            nx=nx,\n",
    "            ny=nx,\n",
    "            scale=survey.pixel_scale.to_value(\"arcsec\")\n",
    "        )\n",
    "    '''else:\n",
    "        star, gsparams, isbright = make_star(entry, survey, survey.get_filter(filt))\n",
    "        max_n_photons = 10_000_000\n",
    "        # 0 means use the flux for n_photons \n",
    "        #mag = -2.5*np.log10(entry[f'flux_{filt}']*1e-9/(1e23*10**(48.6/-2.5)))\n",
    "        mag =entry['mag_'+str(filt)]\n",
    "        flux = mag2counts(mag,survey,filt).to_value(\"electron\")\n",
    "        n_photons = 0 if flux < max_n_photons else max_n_photons\n",
    "        #n_photons = 0 if entry[f'flux_{filt}'] < max_n_photons else max_n_photons\n",
    "        conv_star = galsim.Convolve(star, psf)\n",
    "        im = conv_star.drawImage(\n",
    "            nx=nx,\n",
    "            ny=nx,\n",
    "            scale=survey.pixel_scale.to_value(\"arcsec\"),\n",
    "            method=\"phot\",\n",
    "            n_photons=n_photons,\n",
    "            poisson_flux=True,\n",
    "            maxN=1_000_000,  # shoot in batches this size\n",
    "            rng=grng\n",
    "        )'''\n",
    "        \n",
    "    return im\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata(img_shape, cat, survey, filt, lvl=3, star_cat = None):\n",
    "\n",
    "    \"\"\" Code to format the metadatain to a dict.  It takes the i-band and makes a footprint+bounding boxes\n",
    "    from thresholding to sn*sky_level\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    blend_batch: BTK blend batch\n",
    "        BTK batch of blends\n",
    "    sky_level: float\n",
    "        The background sky level in the i-band\n",
    "    sn: int\n",
    "        The signal-to-noise ratio for thresholding\n",
    "    idx:\n",
    "        The index of the blend in the blend_batch\n",
    "        \n",
    "    Returns\n",
    "        ddict: dict\n",
    "            The dictionary of metadata for the idx'th blend in the batch \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    ddict = {}\n",
    "\n",
    "    ddict[f\"file_name\"] = 'none'\n",
    "    ddict[\"image_id\"] = 0\n",
    "    ddict[\"height\"] = img_shape[0]\n",
    "    ddict[\"width\"] = img_shape[1]\n",
    "    \n",
    "    \n",
    "    t = Table.from_pandas(cat)\n",
    "    #t = cat\n",
    "\n",
    "    n = len(cat)\n",
    "    objs = []\n",
    "    for j in range(n):\n",
    "\n",
    "        obj = t[j]\n",
    "        \n",
    "        #a = math.ceil(obj['size_true']/0.2)*2\n",
    "        #b = math.ceil(obj['size_minor_true']/0.2)*2\n",
    "        x = obj['new_x']\n",
    "        y = obj['new_y']\n",
    "        #mask = make_seg(obj,survey,filt, lvl)\n",
    "        \n",
    "        segs = []\n",
    "        for filt in ['u','g','r','i','z','y']:\n",
    "            im  = make_im(obj, survey, filt, lvl=2, nx=128,ny=128)\n",
    "    \n",
    "            imd = np.expand_dims(np.expand_dims(im.array,0),0)\n",
    "            sky_level = mean_sky_level(survey.name, filt).to_value('electron') # gain = 1\n",
    "            segs.append(btk.metrics.utils.get_segmentation(imd, sky_level, sigma_noise=2))\n",
    "\n",
    "        mask = np.clip(np.sum(segs,axis=0), a_min=0, a_max=1)[0][0]\n",
    "        \n",
    "        \n",
    "        #mask=cv2.ellipse(frame, (frame.shape[0]//2,frame.shape[1]//2), (a,b), pa, 0 , 360, (255,0,0), -1)\n",
    "        #frame = np.zeros((dat.shape[1],dat.shape[2]))\n",
    "        #mask=cv2.ellipse(frame, (0,0), (a,b), pa, 0 , 360, (255,0,0), -1)\n",
    "#         print(obj[\"truth_type\"], \": \" ,mask, \"\\n\")\n",
    "        if np.sum(mask)==0:\n",
    "            continue\n",
    "        \n",
    "        bbox = get_bbox(mask)\n",
    "        x0 = bbox[2]\n",
    "        x1 = bbox[3]\n",
    "        y0 = bbox[0]\n",
    "        y1 = bbox[1]\n",
    "        \n",
    "        w = x1-x0\n",
    "        h = y1-y0\n",
    "        \n",
    "        bbox = [x-w/2, y-h/2, w, h]     \n",
    "\n",
    "        redshift = obj['redshift']\n",
    "        obj_id = obj['object_id']\n",
    "        mag_i = obj['i_ab']\n",
    "        et_1 = obj['ellipticity_1_true']\n",
    "        et_2 = obj['ellipticity_2_true']\n",
    "\n",
    "        contours, hierarchy = cv2.findContours(\n",
    "                    (mask).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "                )\n",
    "\n",
    "\n",
    "        segmentation = []\n",
    "        for contour in contours:\n",
    "            # contour = [x1, y1, ..., xn, yn]\n",
    "            contour = contour.flatten()\n",
    "            if len(contour) > 4:\n",
    "                contour[::2] += (int(np.rint(x))-x0-w//2)\n",
    "                contour[1::2] += (int(np.rint(y))-y0-h//2)\n",
    "                #contour[::2] += (int(y)-y0-h//2)\n",
    "                #contour[1::2] += (int(x)-x0-w//2)\n",
    "                \n",
    "                segmentation.append(contour.tolist())\n",
    "        # No valid countors\n",
    "        if len(segmentation) == 0:\n",
    "            print(j)\n",
    "            continue\n",
    "\n",
    "        obj = {\n",
    "            \"bbox\": bbox,\n",
    "            \"area\": w*h,\n",
    "            #\"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"bbox_mode\": 1,\n",
    "            \"segmentation\": segmentation,\n",
    "            \"category_id\": 1 if obj['truth_type'] == 2 else 0,\n",
    "            \"redshift\": redshift,\n",
    "            \"obj_id\": obj_id,\n",
    "            \"mag_i\": mag_i,\n",
    "            \"et_1\": et_1,\n",
    "            \"et_2\": et_2,\n",
    "        }\n",
    "        objs.append(obj)\n",
    "        \n",
    "    \n",
    "    ddict['annotations'] = objs\n",
    "\n",
    "    return ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def convert_to_json(dict_list, output_file, allow_cached=True):\n",
    "    \"\"\"\n",
    "    Converts dataset into COCO format and saves it to a json file.\n",
    "    dataset_name must be registered in DatasetCatalog and in detectron2's standard format.\n",
    "\n",
    "    Args:\n",
    "        dict_list: list of metadata dictionaries\n",
    "        output_file: path of json file that will be saved to\n",
    "        allow_cached: if json file is already present then skip conversion\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Caching COCO format annotations at '{output_file}' ...\")\n",
    "    tmp_file = output_file + \".tmp\"\n",
    "    with open(tmp_file, \"w\") as f:\n",
    "        json.dump(dict_list, f,cls=NpEncoder)\n",
    "    shutil.move(tmp_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 10/10 [00:09<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching COCO format annotations at './datasets/CosmoDC2/dc2_4mdet_fits/temp_test.json' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "survey = btk.survey.get_surveys(\"LSST\")\n",
    "filt = 'i'\n",
    "ddicts = []\n",
    "for img_index in tqdm(range(10), desc=\"Processing Images\"):\n",
    "    outdir = f'./datasets/CosmoDC2/mdet_sims_train/{img_index}/'\n",
    "    img_shape = [510, 510]\n",
    "\n",
    "    cat = pd.read_csv(outdir+ f'sim_mdet_cat.csv')\n",
    "    catr = dcut_reformat(cat, only_bttri=False)\n",
    "    ddict = create_metadata(img_shape, cat, survey, filt, lvl=3, star_cat = None)\n",
    "    ddict['filename'] = outdir\n",
    "    ddicts.append(ddict)\n",
    "\n",
    "convert_to_json(ddicts, './datasets/CosmoDC2/dc2_4mdet_fits/temp.json', allow_cached=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_shurui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
